# Mambaä¸­Aã€BçŸ©é˜µçš„åŠ¨æ€è°ƒæ•´æœºåˆ¶è¯¦è§£

## ğŸ“š ç›®å½•

1. [SSMåŸºç¡€ç†è®º](#1-ssmåŸºç¡€ç†è®º)
2. [Mambaçš„é€‰æ‹©æ€§æœºåˆ¶](#2-mambaçš„é€‰æ‹©æ€§æœºåˆ¶)
3. [Aã€BçŸ©é˜µçš„åŠ¨æ€ç”Ÿæˆ](#3-abçŸ©é˜µçš„åŠ¨æ€ç”Ÿæˆ)
4. [ä»£ç å®ç°è¯¦è§£](#4-ä»£ç å®ç°è¯¦è§£)
5. [å¦‚ä½•å½±å“Aã€BçŸ©é˜µ](#5-å¦‚ä½•å½±å“abçŸ©é˜µ)
6. [å®è·µåº”ç”¨æ¡ˆä¾‹](#6-å®è·µåº”ç”¨æ¡ˆä¾‹)

---

## 1. SSMåŸºç¡€ç†è®º

### **ä¼ ç»ŸçŠ¶æ€ç©ºé—´æ¨¡å‹ï¼ˆState Space Modelï¼‰**

```python
# è¿ç»­æ—¶é—´å½¢å¼ï¼š
dx/dt = AÂ·x(t) + BÂ·u(t)
y(t) = CÂ·x(t) + DÂ·u(t)

# ç¦»æ•£åŒ–åï¼š
h_t = AÂ·h_{t-1} + BÂ·x_t     # çŠ¶æ€æ›´æ–°
y_t = CÂ·h_t                  # è¾“å‡º

å…¶ä¸­ï¼š
- x_t: è¾“å…¥ï¼ˆå¦‚ç‚¹äº‘ç‰¹å¾ï¼‰
- h_t: éšè—çŠ¶æ€
- y_t: è¾“å‡º
- A: çŠ¶æ€è½¬ç§»çŸ©é˜µ (NÃ—N)
- B: è¾“å…¥çŸ©é˜µ (NÃ—D)
- C: è¾“å‡ºçŸ©é˜µ (DÃ—N)
- N: çŠ¶æ€ç»´åº¦
- D: ç‰¹å¾ç»´åº¦
```

### **ä¼ ç»ŸSSMçš„é—®é¢˜**

```python
# é—®é¢˜1ï¼šAã€Bã€Cæ˜¯å›ºå®šçš„å‚æ•°
A = å­¦ä¹ çš„å›ºå®šçŸ©é˜µ  # å¯¹æ‰€æœ‰è¾“å…¥éƒ½ä¸€æ ·
B = å­¦ä¹ çš„å›ºå®šçŸ©é˜µ

# é—®é¢˜2ï¼šæ— æ³•æ ¹æ®è¾“å…¥å†…å®¹è°ƒæ•´
å¯¹äºä¸åŒçš„è¾“å…¥ x1, x2ï¼š
  h1 = AÂ·h0 + BÂ·x1
  h2 = AÂ·h1 + BÂ·x2
  â†‘ ä½¿ç”¨ç›¸åŒçš„Aã€B

# ç±»æ¯”ï¼š
å°±åƒç”¨åŒä¸€ä¸ª"è®°å¿†è§„åˆ™"å¤„ç†æ‰€æœ‰ä¿¡æ¯
â†’ æ— æ³•åŒºåˆ†"é‡è¦ä¿¡æ¯"å’Œ"ä¸é‡è¦ä¿¡æ¯"
```

---

## 2. Mambaçš„é€‰æ‹©æ€§æœºåˆ¶

### **æ ¸å¿ƒåˆ›æ–°ï¼šSelective SSM**

```python
# Mambaçš„å…³é”®ï¼šAã€Bã€Cæ˜¯è¾“å…¥ä¾èµ–çš„ï¼
A_t = f_A(x_t)  # æ ¹æ®è¾“å…¥x_tåŠ¨æ€ç”ŸæˆA
B_t = f_B(x_t)  # æ ¹æ®è¾“å…¥x_tåŠ¨æ€ç”ŸæˆB
C_t = f_C(x_t)  # æ ¹æ®è¾“å…¥x_tåŠ¨æ€ç”ŸæˆC

# çŠ¶æ€æ›´æ–°å˜æˆï¼š
h_t = A_tÂ·h_{t-1} + B_tÂ·x_t
      â†‘ åŠ¨æ€çš„        â†‘ åŠ¨æ€çš„
y_t = C_tÂ·h_t
      â†‘ åŠ¨æ€çš„

# å¥½å¤„ï¼š
- å¯ä»¥æ ¹æ®è¾“å…¥å†…å®¹è°ƒæ•´"è®°å¿†è§„åˆ™"
- å¯¹é‡è¦ä¿¡æ¯ï¼šA_tå¤§ï¼ˆä¿æŒè®°å¿†ï¼‰ï¼ŒB_tå¤§ï¼ˆå¼ºçƒˆå…³æ³¨ï¼‰
- å¯¹ä¸é‡è¦ä¿¡æ¯ï¼šA_tå°ï¼ˆé—å¿˜ï¼‰ï¼ŒB_tå°ï¼ˆå¼±å…³æ³¨ï¼‰
```

### **ç›´è§‚ç†è§£**

```
ä¼ ç»ŸSSMï¼ˆå›ºå®šAã€Bï¼‰:
è¾“å…¥åºåˆ—: [é‡è¦ä¿¡æ¯, å™ªéŸ³, é‡è¦ä¿¡æ¯, å™ªéŸ³]
            â†“        â†“       â†“        â†“
         åŒæ ·çš„Aã€Bå¤„ç†æ‰€æœ‰è¾“å…¥
            â†“
         è¾“å‡ºæ··æ‚äº†é‡è¦ä¿¡æ¯å’Œå™ªéŸ³

Mambaï¼ˆåŠ¨æ€Aã€Bï¼‰:
è¾“å…¥åºåˆ—: [é‡è¦ä¿¡æ¯, å™ªéŸ³, é‡è¦ä¿¡æ¯, å™ªéŸ³]
            â†“        â†“       â†“        â†“
         Aâ†‘Bâ†‘    Aâ†“Bâ†“    Aâ†‘Bâ†‘    Aâ†“Bâ†“
         (å¼ºè®°å¿†) (å¼±è®°å¿†) (å¼ºè®°å¿†) (å¼±è®°å¿†)
            â†“
         è¾“å‡ºä¿ç•™é‡è¦ä¿¡æ¯ï¼Œè¿‡æ»¤å™ªéŸ³
```

---

## 3. Aã€BçŸ©é˜µçš„åŠ¨æ€ç”Ÿæˆ

### **Mamba2çš„æ¶æ„**

åœ¨ä½ çš„ä»£ç ä¸­ä½¿ç”¨çš„æ˜¯Mamba2ï¼Œå…¶åŠ¨æ€ç”Ÿæˆæœºåˆ¶å¦‚ä¸‹ï¼š

```python
# Mamba2çš„å‰å‘è¿‡ç¨‹ï¼ˆç®€åŒ–ï¼‰ï¼š

class Mamba2(nn.Module):
    def __init__(self, d_model, d_state=64, d_conv=4, expand=2, ...):
        """
        Args:
            d_model: è¾“å…¥ç‰¹å¾ç»´åº¦ (D)
            d_state: çŠ¶æ€ç©ºé—´ç»´åº¦ (N)
            d_conv: å·ç§¯æ ¸å¤§å°
            expand: æ‰©å±•å› å­
        """
        self.d_model = d_model
        self.d_state = d_state
        self.d_inner = d_model * expand  # å†…éƒ¨ç»´åº¦
        
        # è¾“å…¥æŠ•å½±ï¼ˆç”Ÿæˆxã€zã€Bã€Cï¼‰
        self.in_proj = nn.Linear(d_model, self.d_inner * 2 + 2 * d_state)
        
        # å·ç§¯ï¼ˆå±€éƒ¨ä¸Šä¸‹æ–‡ï¼‰
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=d_conv,
            groups=self.d_inner,  # depthwise
            padding=d_conv - 1
        )
        
        # AçŸ©é˜µï¼ˆå¯¹æ•°åŸŸï¼Œå¯å­¦ä¹ å‚æ•°ï¼‰
        self.A_log = nn.Parameter(torch.log(torch.rand(d_state)))
        
        # DeltaæŠ•å½±ï¼ˆç”Ÿæˆæ—¶é—´æ­¥é•¿ï¼‰
        self.dt_proj = nn.Linear(d_state, self.d_inner)
        
        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(self.d_inner, d_model)
    
    def forward(self, x):
        """
        Args:
            x: (B, L, D) è¾“å…¥åºåˆ—
        Returns:
            y: (B, L, D) è¾“å‡ºåºåˆ—
        """
        B, L, D = x.shape
        
        # ===== æ­¥éª¤1ï¼šè¾“å…¥æŠ•å½±ï¼Œç”Ÿæˆx_proj, z, B, C =====
        xz_bc = self.in_proj(x)  # (B, L, d_inner*2 + 2*d_state)
        
        # åˆ†å‰²
        x_proj = xz_bc[:, :, :self.d_inner]                    # (B, L, d_inner)
        z = xz_bc[:, :, self.d_inner:self.d_inner*2]           # (B, L, d_inner) é—¨æ§
        B = xz_bc[:, :, self.d_inner*2:self.d_inner*2+d_state] # (B, L, N) â† BçŸ©é˜µï¼
        C = xz_bc[:, :, self.d_inner*2+d_state:]               # (B, L, N) â† CçŸ©é˜µï¼
        
        # ===== æ­¥éª¤2ï¼šå·ç§¯ï¼ˆæ•è·å±€éƒ¨ä¿¡æ¯ï¼‰ =====
        x_conv = self.conv1d(x_proj.transpose(1, 2)).transpose(1, 2)  # (B, L, d_inner)
        x_conv = F.silu(x_conv)  # æ¿€æ´»
        
        # ===== æ­¥éª¤3ï¼šç”ŸæˆDeltaï¼ˆæ—¶é—´æ­¥é•¿ï¼‰ =====
        delta = self.dt_proj(x_conv)  # (B, L, d_inner)
        delta = F.softplus(delta)     # ä¿è¯æ­£æ•°
        
        # ===== æ­¥éª¤4ï¼šç”ŸæˆAçŸ©é˜µ =====
        A = -torch.exp(self.A_log)  # (N,) è´Ÿæ•°ï¼Œä¿è¯ç¨³å®šæ€§
        # æ‰©å±•åˆ°æ¯ä¸ªæ—¶é—´æ­¥
        A_expanded = A.unsqueeze(0).unsqueeze(0).expand(B, L, -1)  # (B, L, N)
        
        # ç¦»æ•£åŒ–ï¼šA_bar = exp(delta * A)
        A_bar = torch.exp(delta.unsqueeze(-1) * A_expanded)  # (B, L, N)
        
        # ===== æ­¥éª¤5ï¼šSSMæ ¸å¿ƒè®¡ç®— =====
        # h_t = A_bar_t * h_{t-1} + B_t * x_t
        # y_t = C_t * h_t
        
        # è¿™é‡Œä½¿ç”¨é«˜æ•ˆçš„å¹¶è¡Œæ‰«æç®—æ³•ï¼ˆparallel scanï¼‰
        y = selective_scan(x_conv, delta, A, B, C)  # (B, L, d_inner)
        
        # ===== æ­¥éª¤6ï¼šé—¨æ§ + è¾“å‡ºæŠ•å½± =====
        y = y * F.silu(z)  # é—¨æ§
        y = self.out_proj(y)  # (B, L, D)
        
        return y
```

### **å…³é”®ç‚¹è§£æ**

#### **AçŸ©é˜µçš„åŠ¨æ€ç”Ÿæˆ**

```python
# 1. Açš„åŸºç¡€å‚æ•°ï¼ˆå¯å­¦ä¹ ï¼‰
self.A_log = nn.Parameter(torch.log(torch.rand(d_state)))
# å½¢çŠ¶ï¼š(N,)ï¼ŒNæ˜¯çŠ¶æ€ç»´åº¦
# æ¯ä¸ªç»´åº¦ä¸€ä¸ªå‚æ•°ï¼Œæ§åˆ¶"åŸºç¡€è®°å¿†å¼ºåº¦"

# 2. æ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´A
delta = self.dt_proj(x_conv)  # (B, L, d_inner)
# deltaæ˜¯"æ—¶é—´æ­¥é•¿"ï¼Œæ ¹æ®è¾“å…¥å†…å®¹å†³å®š
# - å¯¹é‡è¦ä¿¡æ¯ï¼šdeltaå° â†’ ç»†ç²’åº¦å¤„ç†
# - å¯¹ä¸é‡è¦ä¿¡æ¯ï¼šdeltaå¤§ â†’ ç²—ç²’åº¦è·³è¿‡

# 3. ç¦»æ•£åŒ–
A = -torch.exp(self.A_log)      # (N,) åŸºç¡€Aï¼ˆè´Ÿæ•°ï¼‰
A_bar = torch.exp(delta * A)    # (B, L, N) åŠ¨æ€A
# å…³é”®å…¬å¼ï¼šA_bar_t = exp(delta_t * A)
#
# å½“delta_tå°æ—¶ï¼šA_bar_t â‰ˆ 1ï¼ˆå¼ºè®°å¿†ï¼Œä¿æŒçŠ¶æ€ï¼‰
# å½“delta_tå¤§æ—¶ï¼šA_bar_t â‰ˆ 0ï¼ˆå¼±è®°å¿†ï¼Œé‡ç½®çŠ¶æ€ï¼‰
```

#### **BçŸ©é˜µçš„åŠ¨æ€ç”Ÿæˆ**

```python
# BçŸ©é˜µç›´æ¥ä»è¾“å…¥æŠ•å½±å¾—åˆ°
xz_bc = self.in_proj(x)  # (B, L, ...)
B = xz_bc[:, :, start:start+d_state]  # (B, L, N)

# æ¯ä¸ªæ—¶é—´æ­¥çš„Béƒ½ä¸åŒï¼š
# - B_1 = f(x_1)
# - B_2 = f(x_2)
# - ...

# ç‰©ç†æ„ä¹‰ï¼š
# B_tæ§åˆ¶"è¾“å…¥x_tå¯¹çŠ¶æ€çš„å½±å“å¼ºåº¦"
# - B_tå¤§ï¼šå¼ºçƒˆå…³æ³¨å½“å‰è¾“å…¥
# - B_tå°ï¼šå¿½ç•¥å½“å‰è¾“å…¥
```

#### **CçŸ©é˜µçš„åŠ¨æ€ç”Ÿæˆ**

```python
# CçŸ©é˜µä¹Ÿä»è¾“å…¥æŠ•å½±å¾—åˆ°
C = xz_bc[:, :, start+d_state:]  # (B, L, N)

# C_tæ§åˆ¶"ä»çŠ¶æ€h_tåˆ°è¾“å‡ºy_tçš„æ˜ å°„"
# ä¸åŒçš„C_tå¯ä»¥é€‰æ‹©æ€§åœ°"è¯»å–"çŠ¶æ€çš„ä¸åŒéƒ¨åˆ†
```

---

## 4. ä»£ç å®ç°è¯¦è§£

### **å®Œæ•´çš„Mamba2å‰å‘è¿‡ç¨‹**

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange

class SimplifiedMamba2(nn.Module):
    """
    ç®€åŒ–ç‰ˆMamba2ï¼Œæ¸…æ™°å±•ç¤ºAã€BåŠ¨æ€è°ƒæ•´æœºåˆ¶
    """
    
    def __init__(self, d_model, d_state=64, d_conv=4, expand=2):
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_inner = d_model * expand
        
        # è¾“å…¥æŠ•å½±ï¼šx â†’ [x_proj, z, B, C]
        self.in_proj = nn.Linear(
            d_model, 
            self.d_inner * 2 + 2 * d_state
        )
        
        # å·ç§¯
        self.conv1d = nn.Conv1d(
            self.d_inner, self.d_inner,
            kernel_size=d_conv,
            groups=self.d_inner,
            padding=d_conv - 1
        )
        
        # AçŸ©é˜µçš„åŸºç¡€å‚æ•°ï¼ˆå¯¹æ•°åŸŸï¼‰
        A_init = torch.arange(1, d_state + 1, dtype=torch.float32)
        self.A_log = nn.Parameter(torch.log(A_init))
        
        # DeltaæŠ•å½±ï¼ˆç”Ÿæˆæ—¶é—´æ­¥é•¿ï¼‰
        self.dt_proj = nn.Linear(self.d_inner, self.d_inner)
        
        # è¾“å‡ºæŠ•å½±
        self.out_proj = nn.Linear(self.d_inner, d_model)
    
    def forward(self, x, return_intermediates=False):
        """
        Args:
            x: (B, L, D) è¾“å…¥åºåˆ—
            return_intermediates: æ˜¯å¦è¿”å›ä¸­é—´å˜é‡ï¼ˆç”¨äºåˆ†æï¼‰
        """
        B, L, D = x.shape
        
        # ===== 1. è¾“å…¥æŠ•å½± =====
        xz_bc = self.in_proj(x)  # (B, L, d_inner*2 + 2*d_state)
        
        # åˆ†å‰²æˆ4ä¸ªéƒ¨åˆ†
        split_sizes = [self.d_inner, self.d_inner, self.d_state, self.d_state]
        x_proj, z, B_input, C_input = torch.split(xz_bc, split_sizes, dim=-1)
        
        print(f"[è°ƒè¯•] x_projå½¢çŠ¶: {x_proj.shape}")  # (B, L, d_inner)
        print(f"[è°ƒè¯•] zå½¢çŠ¶: {z.shape}")            # (B, L, d_inner) é—¨æ§
        print(f"[è°ƒè¯•] B_inputå½¢çŠ¶: {B_input.shape}")  # (B, L, N) â† åŠ¨æ€Bï¼
        print(f"[è°ƒè¯•] C_inputå½¢çŠ¶: {C_input.shape}")  # (B, L, N) â† åŠ¨æ€Cï¼
        
        # ===== 2. å·ç§¯ï¼ˆå±€éƒ¨ä¸Šä¸‹æ–‡ï¼‰ =====
        x_conv = rearrange(x_proj, 'b l d -> b d l')
        x_conv = self.conv1d(x_conv)[:, :, :L]  # ç§»é™¤padding
        x_conv = rearrange(x_conv, 'b d l -> b l d')
        x_conv = F.silu(x_conv)
        
        # ===== 3. ç”ŸæˆDeltaï¼ˆæ—¶é—´æ­¥é•¿ï¼‰ =====
        delta = self.dt_proj(x_conv)  # (B, L, d_inner)
        delta = F.softplus(delta)     # ä¿è¯æ­£æ•°
        
        print(f"[è°ƒè¯•] deltaèŒƒå›´: [{delta.min():.4f}, {delta.max():.4f}]")
        print(f"[è°ƒè¯•] deltaå‡å€¼: {delta.mean():.4f}")
        
        # ===== 4. ç”ŸæˆAçŸ©é˜µ =====
        A_base = -torch.exp(self.A_log)  # (N,) åŸºç¡€Aï¼ˆè´Ÿæ•°ï¼‰
        print(f"[è°ƒè¯•] A_baseèŒƒå›´: [{A_base.min():.4f}, {A_base.max():.4f}]")
        
        # ç¦»æ•£åŒ–ï¼šA_bar = exp(delta * A)
        # æ‰©å±•ç»´åº¦ä»¥è¿›è¡Œå¹¿æ’­
        # delta: (B, L, d_inner)
        # A_base: (N,)
        # éœ€è¦å¯¹æ¯ä¸ªheadè®¡ç®—
        
        # ç®€åŒ–ï¼šå‡è®¾d_inner = d_state * num_heads
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œç›´æ¥ä½¿ç”¨
        A_bar = torch.exp(
            delta[..., :self.d_state].unsqueeze(-1) * 
            A_base.unsqueeze(0).unsqueeze(0)
        )  # (B, L, N)
        
        print(f"[è°ƒè¯•] A_barèŒƒå›´: [{A_bar.min():.4f}, {A_bar.max():.4f}]")
        print(f"[è°ƒè¯•] A_barå‡å€¼: {A_bar.mean():.4f}")
        
        # ===== 5. SSMæ ¸å¿ƒè®¡ç®—ï¼ˆç®€åŒ–ç‰ˆï¼‰ =====
        # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„sequentialæ‰«æï¼Œå®é™…Mamba2ç”¨å¹¶è¡Œæ‰«æ
        h = torch.zeros(B, self.d_state, device=x.device, dtype=x.dtype)
        outputs = []
        
        for t in range(L):
            # å–å½“å‰æ—¶é—´æ­¥çš„å‚æ•°
            A_t = A_bar[:, t, :]           # (B, N)
            B_t = B_input[:, t, :]         # (B, N)
            C_t = C_input[:, t, :]         # (B, N)
            x_t = x_conv[:, t, :self.d_state]  # (B, N)
            
            # çŠ¶æ€æ›´æ–°ï¼šh_t = A_t âŠ™ h_{t-1} + B_t âŠ™ x_t
            h = A_t * h + B_t * x_t        # (B, N)
            
            # è¾“å‡ºï¼šy_t = C_t âŠ™ h_t
            y_t = C_t * h                   # (B, N)
            
            outputs.append(y_t)
        
        y = torch.stack(outputs, dim=1)  # (B, L, N)
        
        # æ‰©å±•åˆ°d_innerç»´åº¦
        y_full = torch.zeros(B, L, self.d_inner, device=x.device, dtype=x.dtype)
        y_full[:, :, :self.d_state] = y
        
        # ===== 6. é—¨æ§ + è¾“å‡ºæŠ•å½± =====
        y_gated = y_full * F.silu(z)
        output = self.out_proj(y_gated)  # (B, L, D)
        
        if return_intermediates:
            intermediates = {
                'B': B_input,
                'C': C_input,
                'A_bar': A_bar,
                'delta': delta,
                'h_final': h
            }
            return output, intermediates
        
        return output
```

---

## 5. å¦‚ä½•å½±å“Aã€BçŸ©é˜µ

### **æ–¹æ³•1ï¼šé€šè¿‡è¾“å…¥ç‰¹å¾å½±å“ï¼ˆé—´æ¥ï¼‰**

```python
# Aã€Bæ˜¯ä»è¾“å…¥xåŠ¨æ€ç”Ÿæˆçš„
# æ‰€ä»¥æ”¹å˜è¾“å…¥ï¼Œå°±èƒ½æ”¹å˜Aã€B

# ä¾‹å­ï¼šåŠ å…¥ä½ç½®ç¼–ç 
class Mamba_WithPosEncoding(nn.Module):
    def __init__(self, d_model, ...):
        super().__init__()
        self.mamba = Mamba2(d_model, ...)
        self.pos_encoder = nn.Linear(3, d_model)  # ä½ç½®ç¼–ç 
    
    def forward(self, x, coords):
        # åŠ å…¥ä½ç½®ä¿¡æ¯
        pos_embed = self.pos_encoder(coords)
        x_with_pos = x + pos_embed
        
        # ä½ç½®ä¿¡æ¯ä¼šå½±å“Mambaå†…éƒ¨çš„Aã€Bç”Ÿæˆ
        # ä¸åŒä½ç½® â†’ ä¸åŒAã€B â†’ ä¸åŒçš„é€‰æ‹©æ€§
        y = self.mamba(x_with_pos)
        
        return y
```

### **æ–¹æ³•2ï¼šä¿®æ”¹DeltaæŠ•å½±ï¼ˆç›´æ¥æ§åˆ¶Aï¼‰**

```python
class Mamba_CustomDelta(nn.Module):
    """
    è‡ªå®šä¹‰Deltaç”Ÿæˆï¼Œç›´æ¥æ§åˆ¶AçŸ©é˜µçš„è°ƒæ•´
    """
    
    def __init__(self, d_model, d_state, ...):
        super().__init__()
        self.mamba = Mamba2(d_model, d_state, ...)
        
        # é¢å¤–çš„Deltaè°ƒåˆ¶å™¨
        self.delta_modulator = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.GELU(),
            nn.Linear(d_model // 2, 1),  # è¾“å‡ºæ ‡é‡
            nn.Sigmoid()  # èŒƒå›´[0, 1]
        )
    
    def forward(self, x):
        # è®¡ç®—è°ƒåˆ¶å› å­
        modulation = self.delta_modulator(x)  # (B, L, 1)
        
        # Hook Mambaå†…éƒ¨çš„Deltaç”Ÿæˆ
        # ï¼ˆéœ€è¦ä¿®æ”¹Mambaæºç ï¼Œæ·»åŠ hookç‚¹ï¼‰
        
        # ä¼ªä»£ç ï¼š
        # original_delta = mamba.compute_delta(x)
        # modified_delta = original_delta * (1 + modulation)
        # mamba.use_delta(modified_delta)
        
        y = self.mamba(x)
        return y

# ä½¿ç”¨åœºæ™¯ï¼š
# - å¯¹äºé‡è¦åŒºåŸŸï¼ˆå¦‚ç‰™é½¿è¾¹ç•Œï¼‰ï¼Œå¢å¤§delta â†’ ç»†ç²’åº¦å¤„ç†
# - å¯¹äºå¹³å¦åŒºåŸŸï¼ˆå¦‚ç‰™é¾ˆä¸­å¿ƒï¼‰ï¼Œå‡å°delta â†’ ç²—ç²’åº¦è·³è¿‡
```

### **æ–¹æ³•3ï¼šæ¡ä»¶åŒ–Mambaï¼ˆå¤–éƒ¨æ§åˆ¶ï¼‰**

```python
class ConditionalMamba(nn.Module):
    """
    æ¡ä»¶åŒ–Mambaï¼šé€šè¿‡å¤–éƒ¨æ¡ä»¶æ§åˆ¶Aã€B
    """
    
    def __init__(self, d_model, d_state, d_cond):
        """
        Args:
            d_cond: æ¡ä»¶å‘é‡ç»´åº¦
        """
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        
        # åŸå§‹Mamba
        self.mamba = Mamba2(d_model, d_state, ...)
        
        # æ¡ä»¶ç¼–ç å™¨
        self.cond_encoder = nn.Sequential(
            nn.Linear(d_cond, d_model),
            nn.LayerNorm(d_model),
            nn.GELU()
        )
        
        # æ¡ä»¶ â†’ Bã€Cçš„è°ƒåˆ¶
        self.B_modulator = nn.Linear(d_model, d_state)
        self.C_modulator = nn.Linear(d_model, d_state)
    
    def forward(self, x, condition):
        """
        Args:
            x: (B, L, D) è¾“å…¥åºåˆ—
            condition: (B, d_cond) æ¡ä»¶å‘é‡
                ä¾‹å¦‚ï¼šä¸Šä¸‹é¢Œæ ‡è®°ã€æ‚£è€…ä¿¡æ¯ç­‰
        """
        # 1. ç¼–ç æ¡ä»¶
        cond_embed = self.cond_encoder(condition)  # (B, D)
        
        # 2. ç”Ÿæˆè°ƒåˆ¶å› å­
        B_mod = self.B_modulator(cond_embed)  # (B, N)
        C_mod = self.C_modulator(cond_embed)  # (B, N)
        
        # 3. å°†æ¡ä»¶èå…¥è¾“å…¥
        cond_embed_expanded = cond_embed.unsqueeze(1).expand(-1, x.shape[1], -1)
        x_cond = x + cond_embed_expanded
        
        # 4. Mambaå¤„ç†
        # è¿™é‡Œéœ€è¦ä¿®æ”¹Mambaçš„forwardï¼Œä½¿å…¶æ¥å—B_modã€C_mod
        # åœ¨Mambaå†…éƒ¨ï¼š
        #   B_dynamic = B_original * B_mod.unsqueeze(1)
        #   C_dynamic = C_original * C_mod.unsqueeze(1)
        
        y = self.mamba(x_cond)  # ç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦ä¼ å…¥mod
        
        return y

# ä½¿ç”¨ç¤ºä¾‹ï¼š
model = ConditionalMamba(d_model=96, d_state=64, d_cond=16)

# å¯¹äºä¸Šé¢Œç‰™é½¿
condition_upper = torch.tensor([[1, 0, 0, ...]])  # ä¸Šé¢Œæ ‡è®°
y_upper = model(x, condition_upper)
# â†’ Bã€Cä¼šæ ¹æ®"ä¸Šé¢Œ"ç‰¹æ€§è°ƒæ•´

# å¯¹äºä¸‹é¢Œç‰™é½¿
condition_lower = torch.tensor([[0, 1, 0, ...]])  # ä¸‹é¢Œæ ‡è®°
y_lower = model(x, condition_lower)
# â†’ Bã€Cä¼šæ ¹æ®"ä¸‹é¢Œ"ç‰¹æ€§è°ƒæ•´
```

### **æ–¹æ³•4ï¼šå¤šå°ºåº¦AçŸ©é˜µ**

```python
class MultiScaleMamba(nn.Module):
    """
    ä¸åŒå±‚çº§ä½¿ç”¨ä¸åŒçš„Aåˆå§‹åŒ–
    æµ…å±‚ï¼šå°Aï¼ˆçŸ­æœŸè®°å¿†ï¼‰
    æ·±å±‚ï¼šå¤§Aï¼ˆé•¿æœŸè®°å¿†ï¼‰
    """
    
    def __init__(self, d_model, d_state, num_layers):
        super().__init__()
        
        self.layers = nn.ModuleList()
        
        for layer_idx in range(num_layers):
            # æ ¹æ®å±‚çº§è°ƒæ•´Açš„åˆå§‹åŒ–
            if layer_idx < num_layers // 2:
                # æµ…å±‚ï¼šå¿«é€Ÿé—å¿˜ï¼Œå…³æ³¨å±€éƒ¨
                A_scale = 0.5
            else:
                # æ·±å±‚ï¼šæ…¢é€Ÿé—å¿˜ï¼Œå…³æ³¨å…¨å±€
                A_scale = 2.0
            
            mamba_layer = Mamba2WithCustomA(
                d_model, d_state,
                A_init_scale=A_scale
            )
            self.layers.append(mamba_layer)
    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x

class Mamba2WithCustomA(Mamba2):
    """
    è‡ªå®šä¹‰Aåˆå§‹åŒ–çš„Mamba2
    """
    
    def __init__(self, d_model, d_state, A_init_scale=1.0, ...):
        super().__init__(d_model, d_state, ...)
        
        # é‡æ–°åˆå§‹åŒ–A
        A_init = torch.arange(1, d_state + 1, dtype=torch.float32)
        A_init = A_init * A_init_scale  # ç¼©æ”¾
        self.A_log = nn.Parameter(torch.log(A_init))
```

---

## 6. å®è·µåº”ç”¨æ¡ˆä¾‹

### **æ¡ˆä¾‹1ï¼šæ ¹æ®ç‚¹äº‘å¯†åº¦è°ƒæ•´é€‰æ‹©æ€§**

```python
class DensityAwareMamba(nn.Module):
    """
    æ ¹æ®ç‚¹äº‘å±€éƒ¨å¯†åº¦è°ƒæ•´Mambaçš„é€‰æ‹©æ€§
    å¯†é›†åŒºåŸŸï¼ˆå¦‚ç‰™é½¿è¡¨é¢ï¼‰â†’ ç»†ç²’åº¦å¤„ç†
    ç¨€ç–åŒºåŸŸï¼ˆå¦‚è¾¹ç•Œï¼‰â†’ ç²—ç²’åº¦å¤„ç†
    """
    
    def __init__(self, d_model, d_state, k=16):
        super().__init__()
        self.k = k
        self.mamba = Mamba2(d_model, d_state, ...)
        
        # å¯†åº¦ç¼–ç å™¨
        self.density_encoder = nn.Sequential(
            nn.Linear(1, d_model // 4),
            nn.GELU(),
            nn.Linear(d_model // 4, d_model)
        )
    
    def compute_local_density(self, coords):
        """
        è®¡ç®—æ¯ä¸ªç‚¹çš„å±€éƒ¨å¯†åº¦ï¼ˆkè¿‘é‚»è·ç¦»çš„å€’æ•°ï¼‰
        """
        from pointops import knn_query
        
        # kè¿‘é‚»æŸ¥è¯¢
        _, dist = knn_query(self.k, coords, coords)  # (N, k)
        
        # å¹³å‡è·ç¦»
        avg_dist = dist.mean(dim=-1, keepdim=True)  # (N, 1)
        
        # å¯†åº¦ = 1 / å¹³å‡è·ç¦»
        density = 1.0 / (avg_dist + 1e-6)
        
        # å½’ä¸€åŒ–
        density = (density - density.mean()) / (density.std() + 1e-6)
        
        return density
    
    def forward(self, x, coords):
        """
        Args:
            x: (N, D) ç‰¹å¾
            coords: (N, 3) åæ ‡
        """
        # 1. è®¡ç®—å±€éƒ¨å¯†åº¦
        density = self.compute_local_density(coords)  # (N, 1)
        
        # 2. ç¼–ç å¯†åº¦
        density_embed = self.density_encoder(density)  # (N, D)
        
        # 3. èå…¥ç‰¹å¾
        x_enhanced = x + 0.1 * density_embed
        
        # 4. Mambaå¤„ç†
        # å¯†åº¦ä¿¡æ¯ä¼šå½±å“å†…éƒ¨çš„deltaç”Ÿæˆï¼š
        # - é«˜å¯†åº¦åŒºåŸŸ â†’ å°delta â†’ ç»†ç²’åº¦
        # - ä½å¯†åº¦åŒºåŸŸ â†’ å¤§delta â†’ ç²—ç²’åº¦
        y = self.mamba(x_enhanced.unsqueeze(0)).squeeze(0)
        
        return y
```

### **æ¡ˆä¾‹2ï¼šä»»åŠ¡æ„ŸçŸ¥çš„Aã€Bè°ƒæ•´**

```python
class TaskAdaptiveMamba(nn.Module):
    """
    æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´Mambaçš„è¡Œä¸º
    åˆ†å‰²ä»»åŠ¡ï¼šå¼ºè°ƒå±€éƒ¨ç»†èŠ‚ â†’ å°Aï¼ˆçŸ­æœŸè®°å¿†ï¼‰
    åˆ†ç±»ä»»åŠ¡ï¼šå…³æ³¨å…¨å±€å½¢çŠ¶ â†’ å¤§Aï¼ˆé•¿æœŸè®°å¿†ï¼‰
    """
    
    def __init__(self, d_model, d_state):
        super().__init__()
        
        # å¤šä¸ªä»»åŠ¡ç‰¹å®šçš„Mamba
        self.mamba_seg = Mamba2WithCustomA(
            d_model, d_state,
            A_init_scale=0.3  # åˆ†å‰²ï¼šçŸ­æœŸè®°å¿†
        )
        
        self.mamba_cls = Mamba2WithCustomA(
            d_model, d_state,
            A_init_scale=3.0  # åˆ†ç±»ï¼šé•¿æœŸè®°å¿†
        )
        
        # ä»»åŠ¡èåˆ
        self.task_fusion = nn.Linear(d_model * 2, d_model)
    
    def forward(self, x, task='both'):
        """
        Args:
            task: 'seg', 'cls', æˆ– 'both'
        """
        if task == 'seg':
            return self.mamba_seg(x)
        elif task == 'cls':
            return self.mamba_cls(x)
        else:  # both
            y_seg = self.mamba_seg(x)
            y_cls = self.mamba_cls(x)
            y_fused = self.task_fusion(
                torch.cat([y_seg, y_cls], dim=-1)
            )
            return y_fused
```

### **æ¡ˆä¾‹3ï¼šåºåˆ—åŒ–æ„ŸçŸ¥çš„Deltaè°ƒæ•´**

```python
class SequenceAwareMamba(nn.Module):
    """
    æ ¹æ®åºåˆ—åŒ–åç›¸é‚»ç‚¹çš„å®é™…ç©ºé—´è·ç¦»è°ƒæ•´Delta
    ç›¸é‚»ç‚¹å¾ˆè¿‘ â†’ å°deltaï¼ˆç»†ç²’åº¦ï¼‰
    ç›¸é‚»ç‚¹è¾ƒè¿œ â†’ å¤§deltaï¼ˆè·³è·ƒï¼‰
    """
    
    def __init__(self, d_model, d_state):
        super().__init__()
        self.mamba = Mamba2(d_model, d_state, ...)
        
        # è·ç¦» â†’ deltaè°ƒåˆ¶
        self.dist_to_delta_mod = nn.Sequential(
            nn.Linear(1, d_model // 2),
            nn.GELU(),
            nn.Linear(d_model // 2, 1),
            nn.Softplus()  # ä¿è¯æ­£æ•°
        )
    
    def forward(self, x, coords, order_indices):
        """
        Args:
            x: (N, D) ç‰¹å¾ï¼ˆå·²æ’åºï¼‰
            coords: (N, 3) åæ ‡ï¼ˆæœªæ’åºï¼‰
            order_indices: (N,) æ’åºç´¢å¼•
        """
        # 1. è®¡ç®—ç›¸é‚»ç‚¹è·ç¦»
        ordered_coords = coords[order_indices]
        diff = ordered_coords[1:] - ordered_coords[:-1]
        dist = torch.norm(diff, dim=-1, keepdim=True)  # (N-1, 1)
        dist = torch.cat([
            torch.zeros(1, 1, device=dist.device),
            dist
        ], dim=0)  # (N, 1)
        
        # 2. è·ç¦» â†’ deltaè°ƒåˆ¶å› å­
        delta_mod = self.dist_to_delta_mod(dist)  # (N, 1)
        
        # 3. å°†è°ƒåˆ¶ä¿¡æ¯ç¼–ç åˆ°ç‰¹å¾ä¸­
        # Mambaå†…éƒ¨ä¼šæ ¹æ®ç‰¹å¾ç”Ÿæˆdelta
        # è¿™é‡Œé€šè¿‡ä¿®æ”¹ç‰¹å¾é—´æ¥å½±å“delta
        x_modulated = x * (1 + 0.1 * delta_mod)
        
        # 4. Mambaå¤„ç†
        y = self.mamba(x_modulated.unsqueeze(0)).squeeze(0)
        
        return y
```

---

## 7. è°ƒè¯•ä¸å¯è§†åŒ–

### **å¯è§†åŒ–Aã€Bçš„åŠ¨æ€å˜åŒ–**

```python
def visualize_mamba_dynamics(model, x, coords):
    """
    å¯è§†åŒ–Mambaä¸­Aã€Bçš„åŠ¨æ€å˜åŒ–
    """
    import matplotlib.pyplot as plt
    
    # å‰å‘ä¼ æ’­ï¼Œè·å–ä¸­é—´å˜é‡
    with torch.no_grad():
        mamba_layer = model.mamba
        
        # Hookè·å–ä¸­é—´å˜é‡
        intermediates = {}
        
        def hook_fn(name):
            def hook(module, input, output):
                intermediates[name] = output
            return hook
        
        # æ³¨å†Œhookï¼ˆéœ€è¦ä¿®æ”¹Mambaæºç æ·»åŠ hookç‚¹ï¼‰
        # mamba_layer.register_forward_hook(hook_fn('output'))
        
        output, inter = model(x, return_intermediates=True)
    
    # æå–Aã€B
    A_bar = inter['A_bar'].cpu().numpy()  # (B, L, N)
    B = inter['B'].cpu().numpy()           # (B, L, N)
    delta = inter['delta'].cpu().numpy()   # (B, L, d_inner)
    
    # ç»˜å›¾
    fig, axes = plt.subplots(3, 1, figsize=(12, 10))
    
    # 1. A_baréšæ—¶é—´çš„å˜åŒ–
    ax = axes[0]
    im = ax.imshow(A_bar[0].T, aspect='auto', cmap='viridis')
    ax.set_xlabel('æ—¶é—´æ­¥')
    ax.set_ylabel('çŠ¶æ€ç»´åº¦')
    ax.set_title('AçŸ©é˜µçš„åŠ¨æ€å˜åŒ–ï¼ˆé¢œè‰²=è®°å¿†å¼ºåº¦ï¼‰')
    plt.colorbar(im, ax=ax)
    
    # 2. Béšæ—¶é—´çš„å˜åŒ–
    ax = axes[1]
    im = ax.imshow(B[0].T, aspect='auto', cmap='plasma')
    ax.set_xlabel('æ—¶é—´æ­¥')
    ax.set_ylabel('çŠ¶æ€ç»´åº¦')
    ax.set_title('BçŸ©é˜µçš„åŠ¨æ€å˜åŒ–ï¼ˆé¢œè‰²=è¾“å…¥å¼ºåº¦ï¼‰')
    plt.colorbar(im, ax=ax)
    
    # 3. Deltaéšæ—¶é—´çš„å˜åŒ–
    ax = axes[2]
    ax.plot(delta[0, :, 0])  # åªç”»ç¬¬ä¸€ä¸ªç»´åº¦
    ax.set_xlabel('æ—¶é—´æ­¥')
    ax.set_ylabel('Deltaå€¼')
    ax.set_title('æ—¶é—´æ­¥é•¿Deltaçš„åŠ¨æ€å˜åŒ–')
    ax.grid(True)
    
    plt.tight_layout()
    plt.savefig('mamba_dynamics.png', dpi=300)
    
    # åˆ†æ
    print(f"A_barç»Ÿè®¡ï¼š")
    print(f"  å‡å€¼: {A_bar.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {A_bar.std():.4f}")
    print(f"  æ¥è¿‘1çš„æ¯”ä¾‹: {(A_bar > 0.9).mean():.2%}ï¼ˆå¼ºè®°å¿†ï¼‰")
    print(f"  æ¥è¿‘0çš„æ¯”ä¾‹: {(A_bar < 0.1).mean():.2%}ï¼ˆå¼±è®°å¿†ï¼‰")
    
    print(f"\nBç»Ÿè®¡ï¼š")
    print(f"  å‡å€¼: {B.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {B.std():.4f}")
    
    print(f"\nDeltaç»Ÿè®¡ï¼š")
    print(f"  å‡å€¼: {delta.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {delta.std():.4f}")

# ä½¿ç”¨
model = SimplifiedMamba2(d_model=96, d_state=64)
x = torch.randn(1, 100, 96)  # (B, L, D)
coords = torch.randn(100, 3)

visualize_mamba_dynamics(model, x, coords)
```

---

## 8. æ€»ç»“

### **å…³é”®è¦ç‚¹**

1. âœ… **Mambaçš„æ ¸å¿ƒ**ï¼šAã€Bã€Cæ˜¯è¾“å…¥ä¾èµ–çš„ï¼ˆselectiveï¼‰
2. âœ… **Açš„åŠ¨æ€æ€§**ï¼šé€šè¿‡Deltaï¼ˆæ—¶é—´æ­¥é•¿ï¼‰åŠ¨æ€è°ƒæ•´è®°å¿†å¼ºåº¦
3. âœ… **Bçš„åŠ¨æ€æ€§**ï¼šç›´æ¥ä»è¾“å…¥æŠ•å½±ç”Ÿæˆï¼Œæ§åˆ¶è¾“å…¥å…³æ³¨åº¦
4. âœ… **å½±å“æ–¹å¼**ï¼š
   - é—´æ¥ï¼šä¿®æ”¹è¾“å…¥ç‰¹å¾
   - ç›´æ¥ï¼šä¿®æ”¹Deltaç”Ÿæˆé€»è¾‘
   - æ¡ä»¶åŒ–ï¼šåŠ å…¥å¤–éƒ¨æ§åˆ¶ä¿¡å·

### **å®è·µå»ºè®®**

å¯¹äºä½ çš„ç‰™é½¿åˆ†å‰²ä»»åŠ¡ï¼š

```python
# å»ºè®®1ï¼šåŠ å…¥ç©ºé—´ä½ç½®ä¿¡æ¯
# è®©Mambaæ ¹æ®3Dä½ç½®åŠ¨æ€è°ƒæ•´Aã€B
x_with_pos = x + pos_encoder(coords)
y = mamba(x_with_pos)

# å»ºè®®2ï¼šæ ¹æ®ç‚¹äº‘å¯†åº¦è°ƒæ•´
# å¯†é›†åŒºåŸŸç»†ç²’åº¦ï¼Œç¨€ç–åŒºåŸŸç²—ç²’åº¦
density = compute_density(coords)
x_enhanced = x + density_encoder(density)
y = mamba(x_enhanced)

# å»ºè®®3ï¼šå¤šå°ºåº¦Aåˆå§‹åŒ–
# æµ…å±‚çŸ­æœŸè®°å¿†ï¼Œæ·±å±‚é•¿æœŸè®°å¿†
for layer_idx, mamba_layer in enumerate(mamba_layers):
    A_scale = 0.5 + layer_idx * 0.3
    mamba_layer.reset_A(A_scale)
```

å®Œæ•´æ–‡æ¡£å·²ä¿å­˜ï¼ŒåŒ…å«äº†ç†è®ºã€å®ç°å’Œå®è·µæ¡ˆä¾‹ï¼
